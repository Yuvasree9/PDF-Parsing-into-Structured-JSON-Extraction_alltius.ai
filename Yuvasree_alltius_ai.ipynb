{
 "cells": [
  {
   "cell_type": "raw",
   "id": "294aa050-91fe-44a1-9c18-e404a9e50631",
   "metadata": {},
   "source": [
    "PDF Parsing into Structured JSON Extraction"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ed20f18a-48b7-445f-8f09-8ed6b18005f8",
   "metadata": {},
   "source": [
    "Hey there, This is Yuvasree Thupalli.\n",
    "\n",
    "I received the assignment project where I need to parse any PDF into a structured JSON file format which can be downloadable for easy access from alltius.ai.\n",
    "\n",
    "In order to run this program, first you need to install these dependencies using 'pip'.\n",
    "\n",
    "The dependencies are- 'pdfplumber' for text and table extraction and PyMuPDF (fitz) for detecting images (which we treat as charts). \n",
    "\n",
    "'!pip install pdfplumber fitz tkinter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "365c0aae-0917-4556-bba7-3fd2dad3986d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in c:\\users\\heman\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.11.7)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\heman\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pdfplumber) (11.2.0)\n",
      "Requirement already satisfied: pdfminer.six==20250506 in c:\\users\\heman\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pdfplumber) (20250506)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\heman\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\heman\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (46.0.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\heman\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (3.4.1)\n",
      "Requirement already satisfied: cffi>=2.0.0 in c:\\users\\heman\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.13.2 in c:\\users\\heman\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (4.15.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\heman\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfplumber"
   ]
  },
  {
   "cell_type": "raw",
   "id": "176e7386-0f85-48de-a9c4-ccb11fc557e4",
   "metadata": {},
   "source": [
    "Now, we are going to upload any PDF file using Tkinter fieldialog and message box. Later pdfplumber helps for extracting text and tables from the PDF whereas PyMuPDF (fitz) helps for detecting images and charts. At last, after parsing entire sample document which is provided is parsed and saved into a structured JSON file (output.json)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b06dfca-52db-43ac-8375-7194867ff0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "\n",
    "def extract_sections(page):\n",
    "    lines = {}\n",
    "    for char in page.chars:\n",
    "        y0 = round(char['top'], 1)\n",
    "        lines.setdefault(y0, []).append(char)\n",
    "\n",
    "    line_infos = []\n",
    "    for y0 in sorted(lines.keys()):\n",
    "        chars = lines[y0]\n",
    "        text = ''.join(c['text'] for c in chars).strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        font_sizes = [c['size'] for c in chars]\n",
    "        max_font_size = max(font_sizes)\n",
    "        is_bold = any('Bold' in c['fontname'] or 'bold' in c['fontname'] for c in chars)\n",
    "        line_infos.append({\n",
    "            'text': text,\n",
    "            'font_size': max_font_size,\n",
    "            'is_bold': is_bold,\n",
    "            'y0': y0\n",
    "        })\n",
    "    return line_infos\n",
    "\n",
    "def classify_sections(line_infos):\n",
    "    if not line_infos:\n",
    "        return []\n",
    "\n",
    "    font_sizes = sorted(set([line['font_size'] for line in line_infos]), reverse=True)\n",
    "    section_size = font_sizes[0] if len(font_sizes) > 0 else None\n",
    "    sub_section_size = font_sizes[1] if len(font_sizes) > 1 else None\n",
    "\n",
    "    classified = []\n",
    "    for line in line_infos:\n",
    "        if line['font_size'] == section_size and line['is_bold']:\n",
    "            classified.append({'type': 'section', 'text': line['text'], 'y0': line['y0']})\n",
    "        elif sub_section_size and line['font_size'] == sub_section_size and line['is_bold']:\n",
    "            classified.append({'type': 'sub_section', 'text': line['text'], 'y0': line['y0']})\n",
    "        else:\n",
    "            classified.append({'type': 'paragraph', 'text': line['text'], 'y0': line['y0']})\n",
    "    return classified\n",
    "\n",
    "def extract_tables(page):\n",
    "    tables = []\n",
    "    for table in page.extract_tables():\n",
    "        clean_table = []\n",
    "        for row in table:\n",
    "            clean_row = [cell.strip() if cell else \"\" for cell in row]\n",
    "            clean_table.append(clean_row)\n",
    "        tables.append(clean_table)\n",
    "    return tables\n",
    "\n",
    "def extract_charts(doc, page_number):\n",
    "    charts = []\n",
    "    page = doc.load_page(page_number - 1)\n",
    "    images = page.get_images(full=True)\n",
    "    for img in images:\n",
    "        xref = img[0]\n",
    "        charts.append({\n",
    "            \"description\": \"Chart/image detected on page\",\n",
    "            \"image_xref\": xref\n",
    "        })\n",
    "    return charts\n",
    "\n",
    "def parse_pdf(pdf_path):\n",
    "    result = {\"pages\": []}\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        for i, page in enumerate(pdf.pages, start=1):\n",
    "            page_content = []\n",
    "\n",
    "            line_infos = extract_sections(page)\n",
    "            classified = classify_sections(line_infos)\n",
    "\n",
    "            current_section = None\n",
    "            current_sub_section = None\n",
    "            paragraph_buffer = []\n",
    "\n",
    "            def flush_paragraph():\n",
    "                nonlocal paragraph_buffer\n",
    "                if paragraph_buffer:\n",
    "                    text = ' '.join(paragraph_buffer)\n",
    "                    page_content.append({\n",
    "                        \"type\": \"paragraph\",\n",
    "                        \"section\": current_section,\n",
    "                        \"sub_section\": current_sub_section,\n",
    "                        \"text\": text\n",
    "                    })\n",
    "                    paragraph_buffer = []\n",
    "\n",
    "            for item in classified:\n",
    "                if item['type'] == 'section':\n",
    "                    flush_paragraph()\n",
    "                    current_section = item['text']\n",
    "                    current_sub_section = None\n",
    "                elif item['type'] == 'sub_section':\n",
    "                    flush_paragraph()\n",
    "                    current_sub_section = item['text']\n",
    "                else:\n",
    "                    paragraph_buffer.append(item['text'])\n",
    "            flush_paragraph()\n",
    "\n",
    "            tables = extract_tables(page)\n",
    "            for table in tables:\n",
    "                page_content.append({\n",
    "                    \"type\": \"table\",\n",
    "                    \"section\": current_section,\n",
    "                    \"description\": None,\n",
    "                    \"table_data\": table\n",
    "                })\n",
    "\n",
    "            charts = extract_charts(doc, i)\n",
    "            for chart in charts:\n",
    "                page_content.append({\n",
    "                    \"type\": \"chart\",\n",
    "                    \"section\": current_section,\n",
    "                    \"description\": chart.get(\"description\"),\n",
    "                    \"table_data\": None\n",
    "                })\n",
    "\n",
    "            result[\"pages\"].append({\n",
    "                \"page_number\": i,\n",
    "                \"content\": page_content\n",
    "            })\n",
    "\n",
    "    return result\n",
    "\n",
    "def select_pdf_and_save_json():\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "\n",
    "    pdf_path = filedialog.askopenfilename(\n",
    "        title=\"Select PDF file\",\n",
    "        filetypes=[(\"PDF files\", \"*.pdf\")]\n",
    "    )\n",
    "    if not pdf_path:\n",
    "        messagebox.showinfo(\"Cancelled\", \"No PDF file selected. Exiting.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        data = parse_pdf(pdf_path)\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"Failed to extract PDF content:\\n{e}\")\n",
    "        return\n",
    "\n",
    "    json_path = filedialog.asksaveasfilename(\n",
    "        title=\"Save JSON file\",\n",
    "        defaultextension=\".json\",\n",
    "        filetypes=[(\"JSON files\", \"*.json\")]\n",
    "    )\n",
    "    if not json_path:\n",
    "        messagebox.showinfo(\"Cancelled\", \"No save location selected. Exiting.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "        messagebox.showinfo(\"Success\", f\"JSON saved successfully:\\n{json_path}\")\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"Failed to save JSON file:\\n{e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    select_pdf_and_save_json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f147d4fe-b1a9-4f35-a643-2112a323c18a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
